{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatting",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1m5MMibx8bdv-NTMgHekt85IVzEfBOgCz",
      "authorship_tag": "ABX9TyPIXreknXqHphnKWbJdeHl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjundev/Chatting-Generator/blob/main/chatting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출처 : https://jsideas.net/kakao_rnn/"
      ],
      "metadata": {
        "id": "LOoJnYiojCNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "metadata": {
        "id": "-jL3qUuiixUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "jwGyjnNMj38r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "BInLy6NVb4VW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = glob.glob(\"./drive/MyDrive/data/*.txt\")"
      ],
      "metadata": {
        "id": "mVnIxOOQtY15"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_das_file(a_file):\n",
        "    sentence_list = []\n",
        "    f = open(a_file, 'r', encoding=\"utf-8\")\n",
        "    for line in f:\n",
        "        if len(line.split(\" \")) == 4:\n",
        "            if (\"https\" not in line) & (\"사진\" not in line) & (\"이모티콘\" not in line) & (\"#\" not in line):\n",
        "                person = line.split(\" \")[0][1:-1]\n",
        "\n",
        "            line = line.split(\" \")[3].strip('\\n')\n",
        "\n",
        "            a_dict = {'person': person, 'line': line}\n",
        "            sentence_list.append(a_dict)\n",
        "    print(\"read_das_file 완료\")\n",
        "    return sentence_list\n",
        "\n",
        "sentence_lists = [read_das_file(file) for file in files]\n",
        "\n",
        "## 리스트를 평평하게 만든다.\n",
        "sentence_list = [sentence for sublist in sentence_lists for sentence in sublist]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N38J6b-PtbkR",
        "outputId": "b47eaff1-a56f-4327-f215-c833e6d2d585"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read_das_file 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미 pickle 만들어둠\n",
        "\n",
        "df = pd.DataFrame(sentence_list)\n",
        "df['counter'] = 0\n",
        "\n",
        "counter = 0\n",
        "\n",
        "## 현재 화자와 다음 화자가 다른 경우, counter를 증가시킨다.\n",
        "for i in range(1, len(df)):\n",
        "    current_person = df.loc[i]['person']\n",
        "    next_person = df.loc[i-1]['person']\n",
        "    if current_person != next_person:\n",
        "        counter = counter + 1\n",
        "    df.at[i, 'counter'] = counter\n",
        "\n",
        "## 화자와 counter를 더해 message_idx를 만든다.\n",
        "df['message_idx'] = df.person.astype(str) + \"-\" + df.counter.astype(str)\n",
        "\n",
        "## counter와 message_idx로 그룹바이하여 문장을 리스트로 묶는다.\n",
        "parsed = pd.DataFrame(df.groupby(['counter', 'message_idx'])['line'].apply(list))\n",
        "\n",
        "## 묶은 라인을 하나의 스트링으로 합치고 줄바꿈을 마지막에 붙인다.\n",
        "parsed['line2'] = parsed['line'].map(lambda x: \" \".join(x).replace(\"\\n\", \"\") + \"\\n\")\n",
        "\n",
        "## 문장을 pickle로 저장한다.\n",
        "final_text = parsed['line2'].values\n",
        "with open(\"das_data_parsed.pickle\", \"wb\") as f:\n",
        "    pickle.dump(final_text, f)\n"
      ],
      "metadata": {
        "id": "FbrTvsSTtp3x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 데이터 준비\n",
        "import time\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "data = pickle.load(open(\"./drive/MyDrive/das_data_parsed.pickle\", \"rb\"))\n",
        "text = \"\".join(data)\n",
        "\n",
        "text[:100]\n",
        "\n",
        "## 사진이 왜케 많냐 저게 다 기본 8년전 7년전 이렇다\\n나랑 자주놀앗구먼ㅋㅋ\\n싸이월드 사진첩 다운로드 받는중인데 이쁜게 많네\\n일시: 5/3(화) 2시 304호  휴. 비도 오\n",
        "\n",
        "\n",
        "## 모든 문자셋을 만든다.\n",
        "vocab = set(text)\n",
        "\n",
        "## 문자셋의 문자에 고유한 숫자를 매긴다. \n",
        "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "\n",
        "## 그 반대로 숫자에 문자를 대응시킨다. \n",
        "## 이렇게 함으로써 신경망을 학습시키고, \n",
        "## 신경망에서 생성한 숫자 시퀀스를 다시 문자열로 변환할 수 있다.\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
        "\n",
        "chars[:100]\n",
        "## array([841,  509, 1077,  455, ... 1408,  785,  150, 341], dtype=int32)\n",
        "## 이렇게 문자가 숫자로 변환된다.\n",
        "\n",
        "num_of_all_chars = np.max(chars) + 1\n",
        "print(\"글자 가짓수: %d\" %num_of_all_chars)\n",
        "## 글자 가짓수: 1862"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL5_dICYuWRR",
        "outputId": "919fb173-4a86-4206-fd31-9ec62691f599"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "글자 가짓수: 2033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 학습 & 검증 batch 준비\n",
        "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
        "    \"\"\"\n",
        "    문자 데이터를 학습 & 검증 데이터로 분리\n",
        "    \n",
        "    파라미터\n",
        "    ------\n",
        "    chars: 문자열 배열\n",
        "    batch_size: 각 배치 크기\n",
        "    num_steps: 입력 데이터에 넣을 시퀀스 스텝 수\n",
        "    split_frac: 트레이닝셋에 넣을 데이터 비율\n",
        "    \n",
        "    returns train_x, train_y, val_x, val_y\n",
        "    \"\"\"\n",
        "    \n",
        "    slice_size = batch_size * num_steps\n",
        "    n_batches = int(len(chars) / slice_size)\n",
        "    \n",
        "    # 배치로 나누고 난 후 남은 잔챙이는 버리자\n",
        "    # y는 x에 1을 더한다. 왜냐하면 캐릭터 단위로 하나씩 미는 RNN이므로.\n",
        "    x = chars[: n_batches * slice_size]\n",
        "    y = chars[1: n_batches * slice_size + 1]\n",
        "    \n",
        "    # 배치 크기에 따라 데이터를 자르고, 2차원 매트릭스로 쌓는다.\n",
        "    x = np.stack(np.split(x, batch_size))\n",
        "    y = np.stack(np.split(y, batch_size))\n",
        "    \n",
        "    # 이제 x와 y는 batch_size x n_batches*num_steps로 된 2차원 배열임\n",
        "    \n",
        "    # 이를 학습과 검증 셋으로 나눈다.\n",
        "    split_idx = int(n_batches * split_frac)\n",
        "    train_x, train_y = x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
        "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
        "    \n",
        "    return train_x, train_y, val_x, val_y\n",
        "\n",
        "## 이런식으로 자를 수 있다.\n",
        "## train_x, train_y, val_x, val_y = split_data(chars, 10, 50)\n",
        "\n",
        "## 배치를 생성하는 함수를 만든다.\n",
        "def get_batch(arrs, num_steps):\n",
        "    \"\"\"\n",
        "    함수가 호출될때마다 배치를 리턴한다.\n",
        "    \n",
        "    파라미터\n",
        "    ------\n",
        "    arrs: 전체 배열\n",
        "    num_steps: 입력 데이터에 넣을 시퀀스 스텝 수\n",
        "    \"\"\"\n",
        "    batch_size, slice_size = arrs[0].shape\n",
        "    \n",
        "    n_batches = int(slice_size / num_steps)\n",
        "    for b in range(n_batches):\n",
        "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
      ],
      "metadata": {
        "id": "MCZoPTmpudeh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. RNN 모델 만들기\n",
        "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
        "             learning_rate=0.001, grad_clip=5, sampling=False):\n",
        "    \"\"\"\n",
        "    RNN 모델을 만든다.\n",
        "    \n",
        "    파라미터\n",
        "    ------\n",
        "    num_classes: 문자 가짓수\n",
        "    batch_size: 배치 크기\n",
        "    num_steps: 입력 데이터로 쓸 시퀀스 스텝 수\n",
        "    lstm_size: LSTM 셀의 유닛 수\n",
        "    num_layers: LSTM 레이어 갯수\n",
        "    learning_rate: 학습 속도 알파\n",
        "    grad_clip: gradient가 폭증하거나 사라지는 것을 막아주는 임계값\n",
        "    sampling: True인 경우, batch_size와 num_steps를 1로 지정\n",
        "    \"\"\"\n",
        "    \n",
        "    if sampling == True:\n",
        "        batch_size, num_steps = 1, 1\n",
        "        \n",
        "    # 그래프를 초기화\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # 입력 & 타겟 변수 선언\n",
        "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
        "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
        "    \n",
        "    # dropout 레이어를 위한 값 설정\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    # 입력 및 타겟 변수를 원핫인코딩\n",
        "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
        "    y_one_hot = tf.one_hot(targets, num_classes)\n",
        "    \n",
        "    ### RNN 레이어를 만든다\n",
        "    # 기본 LSTM cell을 사용함\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    \n",
        "    # dropout 레이어 추가\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "    \n",
        "    # LSTM 레이어를 여러개 쌓아서 딥러닝 모델 구축\n",
        "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
        "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    ### RNN 레이어에 데이터를 넣는다\n",
        "    rnn_inputs = [tf.squeeze(i, axis=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
        "    \n",
        "    # RNN에 각 시퀀스를 넣고 결과를 출력\n",
        "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
        "    final_state = state\n",
        "    \n",
        "    # output의 형태를 바꿈\n",
        "    seq_output = tf.concat(outputs, axis=1)\n",
        "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
        "    \n",
        "    # RNN 결과를 softmax 레이어에 연결\n",
        "    with tf.variable_scope('softmax'):\n",
        "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
        "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
        "    \n",
        "    # 행렬 계산\n",
        "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "    \n",
        "    # 소프트맥스에 넣어 다음 문자의 확률을 계산 (총합은 1)\n",
        "    preds = tf.nn.softmax(logits, name='prediction')\n",
        "    \n",
        "    # target의 형태를 변형해서 logits에 맞춤\n",
        "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
        "    cost = tf.reduce_mean(loss)\n",
        "    \n",
        "    # AdamOptimizer와 gradient clipping으로 최적화\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
        "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
        "    \n",
        "    # 노드 출력\n",
        "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
        "                   'keep_prob', 'cost', 'preds', 'optimizer']\n",
        "    \n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "    \n",
        "    return graph"
      ],
      "metadata": {
        "id": "gyPpUc0WujDY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 파라미터 정의\n",
        "batch_size = 100\n",
        "num_steps = 100\n",
        "lstm_size = 512\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "keep_prob = 0.5"
      ],
      "metadata": {
        "id": "-tp7hik2unTQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습\n",
        "epochs = 50\n",
        "\n",
        "# N 이터레이션마다 저장\n",
        "save_every_n = 100\n",
        "\n",
        "# 데이터 준비\n",
        "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
        "print(len(vocab))\n",
        "model = build_rnn(len(vocab),\n",
        "                 batch_size=batch_size,\n",
        "                 num_steps=num_steps,\n",
        "                 learning_rate=learning_rate,\n",
        "                 lstm_size=lstm_size,\n",
        "                 num_layers=num_layers)\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    n_batches = int(train_x.shape[1]/num_steps)\n",
        "    iterations = n_batches * epochs\n",
        "    \n",
        "    # 에폭을 돌면서\n",
        "    for e in range(epochs):\n",
        "        \n",
        "        # 네트워크를 학습시킨다\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        loss = 0\n",
        "\n",
        "        # get_batch 함수로 train 데이터셋을 생성하여 모델에 입력한다.\n",
        "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
        "            iteration = e*n_batches + b\n",
        "            start = time.time()\n",
        "            feed = {model.inputs: x,\n",
        "                    model.targets: y,\n",
        "                    model.keep_prob: keep_prob,\n",
        "                    model.initial_state: new_state}\n",
        "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer],\n",
        "                                               feed_dict=feed)\n",
        "            loss += batch_loss\n",
        "            end = time.time()\n",
        "            print('Epoch {}/{}'.format(e+1, epochs),\n",
        "                 'Iteration {}/{}'.format(iteration, iterations),\n",
        "                 'Traning loss: {:.4f}'.format(loss/b),\n",
        "                 '{:.4f} sec/batch'.format((end-start)))\n",
        "            \n",
        "            # n 이터레이션 마다, 혹은 마지막 이터레이션에 도달하면\n",
        "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
        "                # 성능을 체크한다.\n",
        "                val_loss = []\n",
        "                new_state = sess.run(model.initial_state)\n",
        "                for x, y in get_batch([val_x, val_y], num_steps):\n",
        "                    feed = {model.inputs: x,\n",
        "                            model.targets: y,\n",
        "                            model.keep_prob: 1.,\n",
        "                            model.initial_state: new_state}\n",
        "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
        "                    val_loss.append(batch_loss)\n",
        "                    \n",
        "                print('Validation loss:', np.mean(val_loss), '체크포인트 저장!')\n",
        "                saver.save(sess, \"checkpoints_parsed/i{}_l{}_v{:3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))\n",
        "\n",
        "## Epoch 1/20 Iteration 1/660 Traning loss: 7.5287 8.4289 sec/batch\n",
        "## Epoch 1/20 Iteration 2/660 Traning loss: 7.4979 7.2635 sec/batch\n",
        "## Epoch 1/20 Iteration 3/660 Traning loss: 7.3909 7.2701 sec/batch\n",
        "## ...\n",
        "## Epoch 3/20 Iteration 99/660 Traning loss: 5.0322 7.1951 sec/batch\n",
        "## Epoch 4/20 Iteration 100/660 Traning loss: 5.1010 7.1860 sec/batch\n",
        "## Validation loss: 4.94154 체크포인트 저장!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "aNuUt32Puq7g",
        "outputId": "3e9a7de1-d78c-4e8f-ea48-901f8064bfa9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 Iteration 729/1150 Traning loss: 1.7215 10.2100 sec/batch\n",
            "Epoch 32/50 Iteration 730/1150 Traning loss: 1.7207 10.0790 sec/batch\n",
            "Epoch 32/50 Iteration 731/1150 Traning loss: 1.7146 9.9890 sec/batch\n",
            "Epoch 32/50 Iteration 732/1150 Traning loss: 1.7113 10.1040 sec/batch\n",
            "Epoch 32/50 Iteration 733/1150 Traning loss: 1.7120 10.4899 sec/batch\n",
            "Epoch 32/50 Iteration 734/1150 Traning loss: 1.7110 9.8694 sec/batch\n",
            "Epoch 32/50 Iteration 735/1150 Traning loss: 1.7082 10.2435 sec/batch\n",
            "Epoch 32/50 Iteration 736/1150 Traning loss: 1.7098 9.8747 sec/batch\n",
            "Epoch 33/50 Iteration 737/1150 Traning loss: 1.7454 10.1248 sec/batch\n",
            "Epoch 33/50 Iteration 738/1150 Traning loss: 1.7155 10.2601 sec/batch\n",
            "Epoch 33/50 Iteration 739/1150 Traning loss: 1.7133 10.3562 sec/batch\n",
            "Epoch 33/50 Iteration 740/1150 Traning loss: 1.7304 10.1535 sec/batch\n",
            "Epoch 33/50 Iteration 741/1150 Traning loss: 1.7398 10.4078 sec/batch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b38ce94dea64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     40\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer],\n\u001b[0;32m---> 41\u001b[0;31m                                                feed_dict=feed)\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_top_n(preds, vocab_size, top_n=5):\n",
        "    p = np.squeeze(preds)\n",
        "    p[np.argsort(p)[:-top_n]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    c = np.random.choice(512, 1, p=p)[0]\n",
        "    return c\n",
        "\n",
        "## 체크포인트에 저장된 모델을 불러와서 새로운 텍스트를 생성한다. \n",
        "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"안녕 \"):\n",
        "    samples = [c for c in prime]\n",
        "    model = build_rnn(len(vocab), lstm_size=lstm_size, sampling=True)\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, checkpoint)\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        for c in prime:\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0, 0] = vocab_to_int[c]\n",
        "            feed = {model.inputs: x,\n",
        "                   model.keep_prob: 1.,\n",
        "                   model.initial_state: new_state}\n",
        "            preds, new_state = sess.run([model.preds, model.final_state],\n",
        "                                       feed_dict=feed)\n",
        "            \n",
        "        c = pick_top_n(preds, len(vocab))\n",
        "        samples.append(int_to_vocab[c])\n",
        "        \n",
        "        for i in range(n_samples):\n",
        "            x[0, 0] = c\n",
        "            feed = {model.inputs:x,\n",
        "                   model.keep_prob: 1.,\n",
        "                   model.initial_state: new_state}\n",
        "            \n",
        "            preds, new_state = sess.run([model.preds, model.final_state],\n",
        "                                       feed_dict=feed)\n",
        "            \n",
        "            c = pick_top_n(preds, len(vocab))\n",
        "            samples.append(int_to_vocab[c])\n",
        "            \n",
        "        return ''.join(samples)\n",
        "\n",
        "## 이런식으로 모델을 불러와서 실행한다.\n",
        "checkpoint = \"./checkpoints_parsed/i700_l512_v1.311343.ckpt\"\n",
        "samp = sample(checkpoint, 100, lstm_size, len(vocab), prime=\"경\")\n",
        "print(samp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcLlXsnJO0r6",
        "outputId": "e177522a-db51-4bae-ad67-00911bf652ea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f854b4a03d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f854b4a03d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f854b4a03d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f854b4a03d0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>>: AttributeError: module 'gast' has no attribute 'Str'\n",
            "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>>: AttributeError: module 'gast' has no attribute 'Str'\n",
            "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>>: AttributeError: module 'gast' has no attribute 'Str'\n",
            "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f854b4fc090>>: AttributeError: module 'gast' has no attribute 'Str'\n",
            "INFO:tensorflow:Restoring parameters from ./checkpoints_parsed/i700_l512_v1.311343.ckpt\n",
            "경이이이이이?이가진?이이진이이?진이이이진이진진이나이이?이이가?가이이진이이이나이?나진나진??이이진나이가이진나나진이진이이??진이진가진가진이가?이?이이이이이진이??이진이가이이?진?이이이이나\n"
          ]
        }
      ]
    }
  ]
}